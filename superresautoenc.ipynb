{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-13T15:43:16.262842Z","iopub.status.busy":"2024-10-13T15:43:16.262431Z","iopub.status.idle":"2024-10-13T15:43:24.012932Z","shell.execute_reply":"2024-10-13T15:43:24.011362Z","shell.execute_reply.started":"2024-10-13T15:43:16.262807Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as f\n","import torch.optim as optim\n","import torchvision\n","from torchvision.transforms import v2\n","import random\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from skimage.metrics import structural_similarity as ssim"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:24.015645Z","iopub.status.busy":"2024-10-13T15:43:24.015149Z","iopub.status.idle":"2024-10-13T15:43:24.020976Z","shell.execute_reply":"2024-10-13T15:43:24.019474Z","shell.execute_reply.started":"2024-10-13T15:43:24.015612Z"},"trusted":true},"outputs":[],"source":["from skimage.metrics import structural_similarity as ssim\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:24.023068Z","iopub.status.busy":"2024-10-13T15:43:24.022540Z","iopub.status.idle":"2024-10-13T15:43:24.035274Z","shell.execute_reply":"2024-10-13T15:43:24.034064Z","shell.execute_reply.started":"2024-10-13T15:43:24.023010Z"},"trusted":true},"outputs":[],"source":["dataset_path = \"/kaggle/input/image-super-resolution/dataset\"\n","train= dataset_path+\"/train/\"\n","val = dataset_path+ \"/val/\"\n","train_hires= train+ \"high_res/\"\n","train_lowres= train + \"low_res/\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:24.036959Z","iopub.status.busy":"2024-10-13T15:43:24.036619Z","iopub.status.idle":"2024-10-13T15:43:24.521805Z","shell.execute_reply":"2024-10-13T15:43:24.520623Z","shell.execute_reply.started":"2024-10-13T15:43:24.036930Z"},"trusted":true},"outputs":[],"source":["hiresimgs= os.listdir(train_hires)\n","lowresimgs= os.listdir(train_lowres)\n","\n","len(hiresimgs), len(lowresimgs)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:24.524698Z","iopub.status.busy":"2024-10-13T15:43:24.524349Z","iopub.status.idle":"2024-10-13T15:43:25.464217Z","shell.execute_reply":"2024-10-13T15:43:25.463096Z","shell.execute_reply.started":"2024-10-13T15:43:24.524670Z"},"trusted":true},"outputs":[],"source":["for i in range(2):\n","    hipath= train_hires+ hiresimgs[i]\n","    lowpath= train_lowres+ lowresimgs[i]\n","    \n","    hi= np.array(Image.open(hipath))\n","    lo= np.array(Image.open(lowpath))\n","    \n","    print(hi.shape, lo.shape)\n","    ax= plt.subplot(121)\n","    ax.imshow(hi)\n","    ax.set_title(\"HighRes\")\n","\n","    ax1= plt.subplot(122)\n","    ax1.imshow(lo)\n","    ax1.set_title(\"LowRes\")\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:25.466281Z","iopub.status.busy":"2024-10-13T15:43:25.465859Z","iopub.status.idle":"2024-10-13T15:43:25.473941Z","shell.execute_reply":"2024-10-13T15:43:25.472833Z","shell.execute_reply.started":"2024-10-13T15:43:25.466244Z"},"trusted":true},"outputs":[],"source":["def calculate_psnr(sr_img, hr_img):\n","    \"\"\"Calculates PSNR between the super-resolution (SR) image and the high-resolution (HR) image.\"\"\"\n","    psnr_value = psnr(hr_img, sr_img, data_range=hr_img.max() - hr_img.min())\n","    return psnr_value\n","\n","def calculate_ssim(sr_img, hr_img):\n","    \"\"\"Calculates SSIM between the super-resolution (SR) image and the high-resolution (HR) image.\"\"\"\n","    ssim_value = ssim(hr_img, sr_img, multichannel=True, data_range=hr_img.max() - hr_img.min())\n","    return ssim_value\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:25.475655Z","iopub.status.busy":"2024-10-13T15:43:25.475286Z","iopub.status.idle":"2024-10-13T15:43:25.487016Z","shell.execute_reply":"2024-10-13T15:43:25.485887Z","shell.execute_reply.started":"2024-10-13T15:43:25.475625Z"},"trusted":true},"outputs":[],"source":["hi.shape == (256, 256, 4) #whytf are there 4 channels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:25.488722Z","iopub.status.busy":"2024-10-13T15:43:25.488374Z","iopub.status.idle":"2024-10-13T15:43:26.361310Z","shell.execute_reply":"2024-10-13T15:43:26.360180Z","shell.execute_reply.started":"2024-10-13T15:43:25.488679Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","# Load the RGBA image\n","image_path = \"/kaggle/input/image-super-resolution/dataset/train/high_res/0.png\"  \n","image = Image.open(image_path).convert(\"RGB\")\n","\n","# Split the image into individual channels\n","r, g, b = image.split()\n","\n","# Plot each channel separately\n","plt.figure(figsize=(10, 5))\n","\n","plt.subplot(1, 4, 1)\n","plt.imshow(r)\n","plt.title('Red Channel')\n","\n","plt.subplot(1, 4, 2)\n","plt.imshow(g)\n","plt.title('Green Channel')\n","\n","plt.subplot(1, 4, 3)\n","plt.imshow(b)\n","plt.title('Blue Channel')\n","\n","# plt.subplot(1, 4, 4)\n","# plt.imshow(a)\n","# plt.title('Alpha Channel')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:26.363126Z","iopub.status.busy":"2024-10-13T15:43:26.362725Z","iopub.status.idle":"2024-10-13T15:43:27.506670Z","shell.execute_reply":"2024-10-13T15:43:27.505400Z","shell.execute_reply.started":"2024-10-13T15:43:26.363092Z"},"trusted":true},"outputs":[],"source":["##checking all sizes\n","\n","for i in range(len(train_hires)):\n","    \n","    hipath= train_hires+ hiresimgs[i]\n","    lowpath= train_lowres+ lowresimgs[i]\n","    \n","    hi= np.array(Image.open(hipath))\n","    lo= np.array(Image.open(lowpath))\n","    \n","    x,y=hi.shape,lo.shape \n","    if x!=(256, 256, 4) or y!=(256, 256, 4):\n","        print(hipath, x.shape, lopath, y.shape)\n","    "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:27.508234Z","iopub.status.busy":"2024-10-13T15:43:27.507902Z","iopub.status.idle":"2024-10-13T15:43:27.516887Z","shell.execute_reply":"2024-10-13T15:43:27.515573Z","shell.execute_reply.started":"2024-10-13T15:43:27.508206Z"},"trusted":true},"outputs":[],"source":["class SRDataset(Dataset):\n","    def __init__(self, dataset_dir,mode=\"train\", transforms=None):\n","        \n","        self.train = dataset_dir + f\"/{mode}/\"\n","        \n","        self.train_hires= self.train+ \"high_res/\"\n","        self.train_lowres= self.train + \"low_res/\"\n","                                      \n","        self.hiimgs = sorted(os.listdir(self.train_hires))\n","        self.lowimgs= sorted(os.listdir(self.train_lowres))\n","                                      \n","        self.tf= transforms\n","\n","    def __len__(self):\n","        return len(self.hiimgs)\n","\n","    def __getitem__(self, idx):\n","    \n","        hiresimg= Image.open(self.train_hires+ self.hiimgs[idx]).convert(\"RGB\")\n","        lowresimg= Image.open(self.train_lowres+ self.lowimgs[idx]).convert(\"RGB\")\n","        \n","        #ensures that the same transform is applied to both images (random flipping should be the same for both)\n","        state = torch.get_rng_state()\n","        hiresimg=self.tf(hiresimg)\n","        torch.set_rng_state(state)\n","        lowresimg= self.tf(lowresimg)\n","        \n","        return lowresimg, hiresimg\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:27.518773Z","iopub.status.busy":"2024-10-13T15:43:27.518337Z","iopub.status.idle":"2024-10-13T15:43:27.529376Z","shell.execute_reply":"2024-10-13T15:43:27.528297Z","shell.execute_reply.started":"2024-10-13T15:43:27.518733Z"},"trusted":true},"outputs":[],"source":["transformations = v2.Compose([\n","#     v2.RandomResizedCrop(size=(256, 256), antialias=True),\n","    v2.RandomHorizontalFlip(p=0.7),\n","    v2.RandomVerticalFlip(p=0.7),\n","#     v2.RandomEqualize(), this made it harder for the model to learn colours\n","    v2.ToImage(),\n","    v2.ToDtype(torch.float32, scale=True)\n","        ])"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:27.531128Z","iopub.status.busy":"2024-10-13T15:43:27.530678Z","iopub.status.idle":"2024-10-13T15:43:27.547396Z","shell.execute_reply":"2024-10-13T15:43:27.546222Z","shell.execute_reply.started":"2024-10-13T15:43:27.531091Z"},"trusted":true},"outputs":[],"source":["test_transformations= v2.Compose([\n","    v2.ToImage(),\n","    v2.ToDtype(torch.float32, scale=True)\n","        ])"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:27.549106Z","iopub.status.busy":"2024-10-13T15:43:27.548719Z","iopub.status.idle":"2024-10-13T15:43:27.724135Z","shell.execute_reply":"2024-10-13T15:43:27.723119Z","shell.execute_reply.started":"2024-10-13T15:43:27.549070Z"},"trusted":true},"outputs":[],"source":["train_dataset = SRDataset(dataset_path, \"train\", transformations)\n","test_dataset= SRDataset(dataset_path, 'val',test_transformations)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:27.729469Z","iopub.status.busy":"2024-10-13T15:43:27.729093Z","iopub.status.idle":"2024-10-13T15:43:27.736702Z","shell.execute_reply":"2024-10-13T15:43:27.735477Z","shell.execute_reply.started":"2024-10-13T15:43:27.729435Z"},"trusted":true},"outputs":[],"source":["len(train_dataset), len(test_dataset)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:27.738250Z","iopub.status.busy":"2024-10-13T15:43:27.737916Z","iopub.status.idle":"2024-10-13T15:43:27.748228Z","shell.execute_reply":"2024-10-13T15:43:27.747119Z","shell.execute_reply.started":"2024-10-13T15:43:27.738221Z"},"trusted":true},"outputs":[],"source":["batch_size = 16\n","dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last= True)\n","test_dataloader= DataLoader(test_dataset, batch_size= batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:27.750105Z","iopub.status.busy":"2024-10-13T15:43:27.749728Z","iopub.status.idle":"2024-10-13T15:43:27.762513Z","shell.execute_reply":"2024-10-13T15:43:27.761361Z","shell.execute_reply.started":"2024-10-13T15:43:27.750070Z"},"trusted":true},"outputs":[],"source":["len(dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:27.764586Z","iopub.status.busy":"2024-10-13T15:43:27.764157Z","iopub.status.idle":"2024-10-13T15:43:28.268529Z","shell.execute_reply":"2024-10-13T15:43:28.267452Z","shell.execute_reply.started":"2024-10-13T15:43:27.764548Z"},"trusted":true},"outputs":[],"source":["low,hi= next(iter(dataloader))\n","\n","low.shape, hi.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:28.270504Z","iopub.status.busy":"2024-10-13T15:43:28.270105Z","iopub.status.idle":"2024-10-13T15:43:30.611730Z","shell.execute_reply":"2024-10-13T15:43:30.610356Z","shell.execute_reply.started":"2024-10-13T15:43:28.270456Z"},"trusted":true},"outputs":[],"source":["num_images_to_plot = 10\n","fig, axes = plt.subplots(num_images_to_plot, 2, figsize=(10, 15))\n","\n","for i, (lowres, highres) in enumerate(dataloader):\n","    for j in range(num_images_to_plot):\n","        lo = lowres[j].permute(1, 2, 0).numpy()\n","        hi = highres[j].permute(1, 2, 0).numpy()\n","        \n","        # Plot noisy image\n","        axes[j, 0].imshow(lo)\n","        axes[j, 0].set_title('LowRes Image')\n","        axes[j, 0].axis('off')\n","        \n","        # Plot original image (target)\n","        axes[j, 1].imshow(hi)\n","        axes[j, 1].set_title('HighRes Image')\n","        axes[j, 1].axis('off')\n","        \n","    break  # Stop after first batch for demonstration\n","\n","plt.tight_layout()\n","# plt.Arrowshow()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:30.614185Z","iopub.status.busy":"2024-10-13T15:43:30.613213Z","iopub.status.idle":"2024-10-13T15:43:32.700469Z","shell.execute_reply":"2024-10-13T15:43:32.699446Z","shell.execute_reply.started":"2024-10-13T15:43:30.614141Z"},"trusted":true},"outputs":[],"source":["num_images_to_plot = 10\n","fig, axes = plt.subplots(num_images_to_plot, 2, figsize=(10, 15))\n","\n","for i, (lowres, highres) in enumerate(dataloader):\n","    for j in range(num_images_to_plot):\n","        lo = lowres[j].permute(1, 2, 0).numpy()\n","        hi = highres[j].permute(1, 2, 0).numpy()\n","        \n","        # Plot noisy image\n","        axes[j, 0].imshow(lo)\n","        axes[j, 0].set_title('LowRes Image')\n","        axes[j, 0].axis('off')\n","        \n","        # Plot original image (target)\n","        axes[j, 1].imshow(hi)\n","        axes[j, 1].set_title('HighRes Image')\n","        axes[j, 1].axis('off')\n","        \n","    break  # Stop after first batch for demonstration\n","\n","plt.tight_layout()\n","plt.show()  # Ensures the images are displayed\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:32.702151Z","iopub.status.busy":"2024-10-13T15:43:32.701803Z","iopub.status.idle":"2024-10-13T15:43:32.709849Z","shell.execute_reply":"2024-10-13T15:43:32.708555Z","shell.execute_reply.started":"2024-10-13T15:43:32.702121Z"},"trusted":true},"outputs":[],"source":["class DoubleConv(nn.Module):\n","    \n","    def __init__(self, in_channels,out_channels):\n","        \n","        super().__init__()\n","        \n","        self.conv = nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1), #we're preserving input height and width\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True),\n","        \n","        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1), #we're preserving input height and width\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True)\n","        )\n","    \n","    def forward(self,x):\n","        return self.conv(x)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:32.711523Z","iopub.status.busy":"2024-10-13T15:43:32.711177Z","iopub.status.idle":"2024-10-13T15:43:32.725955Z","shell.execute_reply":"2024-10-13T15:43:32.724684Z","shell.execute_reply.started":"2024-10-13T15:43:32.711491Z"},"trusted":true},"outputs":[],"source":["class UNET(nn.Module):\n","    \n","    def __init__(self, in_channels=3, out_channels=3, features=[64,128,256,512,1024]):\n","        \n","        super().__init__()\n","        \n","        self.downsamples=nn.ModuleList()\n","        \n","        self.upsamples= nn.ModuleList()\n","        \n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        \n","        #now building the encoder\n","        \n","        for feature in features:\n","            self.downsamples.append(DoubleConv(in_channels, feature))\n","            in_channels=feature\n","\n","        #the decoder\n","        \n","        for feature in reversed(features):\n","            self.upsamples.append(\n","                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n","            )\n","            self.upsamples.append(DoubleConv(feature*2, feature))\n","            \n","        #bottleneck layer\n","        self.bottleneck= DoubleConv(features[-1], features[-1]*2)\n","\n","        #final 1x1 conv to change out_channels\n","        self.final= nn.Conv2d(features[0], out_channels, kernel_size=1)\n","        \n","        \n","    def forward(self,x):\n","        skip_connections=[]\n","\n","        for downsampler in self.downsamples:\n","\n","            x = downsampler(x) #this is the output of each downsampling layer\n","            skip_connections.append(x)\n","            x= self.pool(x)\n","\n","        x= self.bottleneck(x) \n","\n","        skip_connections= skip_connections[::-1]\n","\n","        for i in range(0, len(self.upsamples), 2):\n","\n","            x= self.upsamples[i](x) # this is the conv transpose layer\n","            skipped= skip_connections[i//2]\n","\n","            concat_skipped= torch.cat((skipped,x),dim=1)\n","\n","            x=self.upsamples[i+1](concat_skipped) #this is the double conv layer\n","\n","        return self.final(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:32.727790Z","iopub.status.busy":"2024-10-13T15:43:32.727427Z","iopub.status.idle":"2024-10-13T15:43:32.744593Z","shell.execute_reply":"2024-10-13T15:43:32.742928Z","shell.execute_reply.started":"2024-10-13T15:43:32.727760Z"},"trusted":true},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:32.746260Z","iopub.status.busy":"2024-10-13T15:43:32.745870Z","iopub.status.idle":"2024-10-13T15:43:35.816615Z","shell.execute_reply":"2024-10-13T15:43:35.815222Z","shell.execute_reply.started":"2024-10-13T15:43:32.746229Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(42)\n","\n","model = UNET().to(device)\n","\n","x = torch.randn(1, 3, 256, 256).to(device)\n","    \n","output = model(x)\n","\n","print(\"Final output size:\", output.size())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:35.819199Z","iopub.status.busy":"2024-10-13T15:43:35.818453Z","iopub.status.idle":"2024-10-13T15:43:35.827783Z","shell.execute_reply":"2024-10-13T15:43:35.826162Z","shell.execute_reply.started":"2024-10-13T15:43:35.819163Z"},"trusted":true},"outputs":[],"source":["pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","pytorch_total_params"]},{"cell_type":"markdown","metadata":{},"source":["## VGG Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:35.829829Z","iopub.status.busy":"2024-10-13T15:43:35.829437Z","iopub.status.idle":"2024-10-13T15:43:44.192251Z","shell.execute_reply":"2024-10-13T15:43:44.191167Z","shell.execute_reply.started":"2024-10-13T15:43:35.829798Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchvision.models as models\n","\n","# Specify the path to the uploaded VGG16 weights file\n","model_path = '/kaggle/input/vgg/pytorch/default/1/vgg16-397923af.pth'  # Replace 'your-dataset-name' with the actual name\n","\n","# Load the VGG16 model architecture\n","vgg = models.vgg16()\n","\n","# Load the local weights into the model\n","vgg.load_state_dict(torch.load(model_path))\n","\n","# Set the model to evaluation mode (optional, but recommended for inference)\n","\n","# Now you can use vgg16 for inference or further training\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:44.193730Z","iopub.status.busy":"2024-10-13T15:43:44.193394Z","iopub.status.idle":"2024-10-13T15:43:44.201915Z","shell.execute_reply":"2024-10-13T15:43:44.200594Z","shell.execute_reply.started":"2024-10-13T15:43:44.193701Z"},"trusted":true},"outputs":[],"source":["class VGGPerceptualLoss(nn.Module):\n","\n","\n","    \n","    def __init__(self, feature_extractor, diction, criterion, lambdas):\n","        super(VGGPerceptualLoss, self).__init__()\n","        self.feature_extractor = feature_extractor\n","        self.diction = diction\n","        self.criterion = criterion\n","        self.lambdas = lambdas\n","\n","    def forward(self, x, y):\n","        x_features = self.feature_extractor(x,self.diction)\n","        y_features = self.feature_extractor(y,self.diction)\n","#         print(len(x_features), len(y_features))\n","        loss = 0\n","        for layer_name,_ in self.diction.items():\n","            x_feat = x_features[layer_name]\n","            y_feat = y_features[layer_name]\n","            \n","            loss += self.lambdas[layer_name] * self.criterion(x_feat, y_feat)        \n","        return loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:44.203599Z","iopub.status.busy":"2024-10-13T15:43:44.203267Z","iopub.status.idle":"2024-10-13T15:43:44.220600Z","shell.execute_reply":"2024-10-13T15:43:44.219473Z","shell.execute_reply.started":"2024-10-13T15:43:44.203572Z"},"trusted":true},"outputs":[],"source":["features = list(vgg.features.children())[:28]\n","features = nn.Sequential(*features)\n","layers = {'6':'6','15':'15','27':'27'}\n","vgg_feature_extractor = {'6': features[:7].to(device), '15': features[7:16].to(device), '27': features[16:].to(device)}\n","lambdas = {'6': 0.3, '15': 0.3, '27': 0.4}\n","features"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:44.222516Z","iopub.status.busy":"2024-10-13T15:43:44.222047Z","iopub.status.idle":"2024-10-13T15:43:44.229395Z","shell.execute_reply":"2024-10-13T15:43:44.228230Z","shell.execute_reply.started":"2024-10-13T15:43:44.222458Z"},"trusted":true},"outputs":[],"source":["def feature_extractor(x, dic):\n","    \n","    features = {}\n","    for name, sequence in dic.items():\n","        x= sequence(x)\n","        features[name] = x\n","    \n","    return features   "]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:44.230991Z","iopub.status.busy":"2024-10-13T15:43:44.230636Z","iopub.status.idle":"2024-10-13T15:43:44.241838Z","shell.execute_reply":"2024-10-13T15:43:44.240592Z","shell.execute_reply.started":"2024-10-13T15:43:44.230962Z"},"trusted":true},"outputs":[],"source":["criterion_mse = nn.MSELoss()\n","\n","vgg_tester= VGGPerceptualLoss(feature_extractor, vgg_feature_extractor, criterion_mse,lambdas)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:44.243553Z","iopub.status.busy":"2024-10-13T15:43:44.243214Z","iopub.status.idle":"2024-10-13T15:43:44.282690Z","shell.execute_reply":"2024-10-13T15:43:44.281398Z","shell.execute_reply.started":"2024-10-13T15:43:44.243523Z"},"trusted":true},"outputs":[],"source":["rand1 = torch.rand(8, 3, 256, 256).to(device)\n","rand2= torch.rand(8,3,256,256).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:44.284417Z","iopub.status.busy":"2024-10-13T15:43:44.284095Z","iopub.status.idle":"2024-10-13T15:43:45.048574Z","shell.execute_reply":"2024-10-13T15:43:45.047413Z","shell.execute_reply.started":"2024-10-13T15:43:44.284390Z"},"trusted":true},"outputs":[],"source":["vgg_loss= vgg_tester.forward(highres[0].to(device), highres[0].to(device))\n","print(type(vgg_loss), vgg_loss)"]},{"cell_type":"markdown","metadata":{},"source":["## Training Loop"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:45.050473Z","iopub.status.busy":"2024-10-13T15:43:45.050105Z","iopub.status.idle":"2024-10-13T15:43:45.057322Z","shell.execute_reply":"2024-10-13T15:43:45.055961Z","shell.execute_reply.started":"2024-10-13T15:43:45.050442Z"},"trusted":true},"outputs":[],"source":["criterion = nn.MSELoss() #\n","criterion_vgg = VGGPerceptualLoss(feature_extractor, vgg_feature_extractor, criterion_mse,lambdas).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.0002)"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:45.059821Z","iopub.status.busy":"2024-10-13T15:43:45.058957Z","iopub.status.idle":"2024-10-13T15:43:45.068963Z","shell.execute_reply":"2024-10-13T15:43:45.067743Z","shell.execute_reply.started":"2024-10-13T15:43:45.059781Z"},"trusted":true},"outputs":[],"source":["from skimage.metrics import structural_similarity as ssim\n","\n","# Your function to calculate SSIM with win_size adjustment\n","def calculate_ssim(sr_img, hr_img):\n","    # Ensure win_size is less than or equal to the smallest side of the image\n","    win_size = min(sr_img.shape[0], sr_img.shape[1], 7)  # Example: win_size = 7 if images are >= 7x7\n","    return ssim(sr_img, hr_img, win_size=win_size, multichannel=True)\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:45.070818Z","iopub.status.busy":"2024-10-13T15:43:45.070458Z","iopub.status.idle":"2024-10-13T15:43:45.079602Z","shell.execute_reply":"2024-10-13T15:43:45.078520Z","shell.execute_reply.started":"2024-10-13T15:43:45.070786Z"},"trusted":true},"outputs":[],"source":["num_epochs = 14"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:45.081884Z","iopub.status.busy":"2024-10-13T15:43:45.081154Z","iopub.status.idle":"2024-10-13T15:43:45.091553Z","shell.execute_reply":"2024-10-13T15:43:45.090505Z","shell.execute_reply.started":"2024-10-13T15:43:45.081847Z"},"trusted":true},"outputs":[],"source":["def calculate_ssim(sr_img, hr_img):\n","    # Find the smallest dimension of the image\n","    min_side = min(sr_img.shape[0], sr_img.shape[1])\n","    \n","    # Ensure win_size is an odd number and less than or equal to min_side\n","    win_size = min(min_side, 7)  # Choose the smaller value between min_side and 7\n","    if win_size % 2 == 0:  # Ensure win_size is odd\n","        win_size -= 1\n","    \n","    # Calculate SSIM, setting channel_axis for RGB images and data_range for floating point images\n","    return ssim(sr_img, hr_img, win_size=win_size, channel_axis=-1, data_range=1.0)\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:45.093899Z","iopub.status.busy":"2024-10-13T15:43:45.093440Z","iopub.status.idle":"2024-10-13T15:43:45.102769Z","shell.execute_reply":"2024-10-13T15:43:45.101616Z","shell.execute_reply.started":"2024-10-13T15:43:45.093859Z"},"trusted":true},"outputs":[],"source":["train_losses = []\n","val_losses = []"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:45.104345Z","iopub.status.busy":"2024-10-13T15:43:45.103965Z","iopub.status.idle":"2024-10-13T15:43:45.114276Z","shell.execute_reply":"2024-10-13T15:43:45.113235Z","shell.execute_reply.started":"2024-10-13T15:43:45.104307Z"},"trusted":true},"outputs":[],"source":["def validate(model, dataloader, criterion, device):\n","    \"\"\"Function to validate the model on validation dataset and return the loss.\"\"\"\n","    model.eval()  # Set model to evaluation mode\n","    total_loss = 0.0\n","    with torch.no_grad():  # Disable gradient computation\n","        for lowres, highres in dataloader:\n","            outputs = model(lowres.to(device))\n","            loss = criterion(outputs, highres.to(device))\n","            total_loss += loss.item()\n","    \n","    return total_loss / len(dataloader)  # Average loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T15:43:45.116082Z","iopub.status.busy":"2024-10-13T15:43:45.115727Z"},"trusted":true},"outputs":[],"source":["for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    total_psnr = 0.0\n","    total_ssim = 0.0\n","    print(f\"Epoch [{epoch+1}]\", end='')\n","    \n","    for i, (lowres, highres) in enumerate(dataloader):\n","        optimizer.zero_grad()\n","        \n","        # Forward pass: Generate SR images\n","        outputs = model(lowres.to(device))\n","        \n","        # Calculate loss (MSE + VGG loss)\n","        loss = 4 * criterion(outputs, highres.to(device))  # MSE Loss (VGGLoss can be added too)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","        \n","        # Convert SR and HR images to numpy for PSNR/SSIM calculation\n","        sr_img = outputs.cpu().detach().numpy().transpose(0, 2, 3, 1)  # Shape: (batch, H, W, C)\n","        hr_img = highres.cpu().detach().numpy().transpose(0, 2, 3, 1)  # Shape: (batch, H, W, C)\n","        \n","        # Compute PSNR and SSIM for each image in the batch\n","        batch_psnr = 0\n","        batch_ssim = 0\n","        for j in range(sr_img.shape[0]):\n","            batch_psnr += calculate_psnr(sr_img[j], hr_img[j])\n","            batch_ssim += calculate_ssim(sr_img[j], hr_img[j])\n","        \n","        total_psnr += batch_psnr / sr_img.shape[0]\n","        total_ssim += batch_ssim / sr_img.shape[0]\n","        \n","        if i % 20 == 0:  # Print every 20 mini-batches\n","            print('[%d, %5d] loss: %.6f, PSNR: %.3f, SSIM: %.3f' % \n","                  (epoch + 1, i + 1, running_loss / 10, total_psnr / (i+1), total_ssim / (i+1)))\n","            running_loss = 0.0\n","        else:\n","            print(\"#\", end='')\n","\n","    # Calculate average training loss for this epoch\n","    avg_train_loss = running_loss / len(dataloader)\n","    train_losses.append(avg_train_loss)\n","\n","    # Validate the model after each epoch\n","    val_loss = validate(model, test_dataloader, criterion, device)\n","    val_losses.append(val_loss)\n","    \n","    # After each epoch, print average PSNR and SSIM\n","    avg_psnr = total_psnr / len(dataloader)\n","    avg_ssim = total_ssim / len(dataloader)\n","    print(f\"Epoch [{epoch+1}] completed with Avg PSNR: {avg_psnr:.3f}, Avg SSIM: {avg_ssim:.3f}, Val Loss: {val_loss:.6f}\")\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 5))\n","plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', marker='o')\n","plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', marker='o')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 5))\n","plt.plot(psnr_values, label='PSNR')\n","plt.plot(ssim_values, label='SSIM')\n","plt.title('PSNR and SSIM over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Value')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["batch = next(iter(dataloader))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Get a batch of data from the train set\n","batch = next(iter(dataloader))\n","\n","# Extract noisy images from the batch\n","lowres = batch[0][:5].to(device)  # Select the first 5 images in the batch\n","highres = batch[1][:5].to(device)  # Corresponding original images\n","\n","# Generate denoised images using the model\n","with torch.no_grad():\n","    srimages = model(lowres)\n","    srimages = srimages.view(-1, 3, 256, 256)  # Reshape to match the original image size\n","\n","# Convert torch tensors to numpy arrays\n","lowres = lowres.cpu().numpy()\n","highres = highres.cpu().numpy()\n","srimages = srimages.cpu().numpy()\n","\n","# Plot the images\n","fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 9))\n","\n","for i in range(5):\n","    # Plot noisy images\n","    axes[0, i].imshow(np.transpose(lowres[i], (1, 2, 0)))\n","    axes[0, i].set_title('LowRes Image')\n","    axes[0, i].axis('off')\n","    \n","    # Plot original images\n","    axes[1, i].imshow(np.transpose(highres[i], (1, 2, 0)))\n","    axes[1, i].set_title('HiRes Image')\n","    axes[1, i].axis('off')\n","    \n","    # Plot denoised images\n","    axes[2, i].imshow(np.transpose(srimages[i], (1, 2, 0)))\n","    axes[2, i].set_title('SR Image')\n","    axes[2, i].axis('off')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with torch.no_grad():\n","    total_psnr = 0.0\n","    total_ssim = 0.0\n","    for i, (lowres, highres) in enumerate(test_dataloader):\n","        outputs = model(lowres.to(device))\n","        \n","        sr_img = outputs.cpu().detach().numpy().transpose(0, 2, 3, 1)\n","        hr_img = highres.cpu().detach().numpy().transpose(0, 2, 3, 1)\n","        \n","        batch_psnr = 0\n","        batch_ssim = 0\n","        for j in range(sr_img.shape[0]):\n","            batch_psnr += calculate_psnr(sr_img[j], hr_img[j])\n","            batch_ssim += calculate_ssim(sr_img[j], hr_img[j])\n","        \n","        total_psnr += batch_psnr / sr_img.shape[0]\n","        total_ssim += batch_ssim / sr_img.shape[0]\n","    \n","    avg_psnr = total_psnr / len(test_dataloader)\n","    avg_ssim = total_ssim / len(test_dataloader)\n","    print(f\"Test Set Evaluation - Avg PSNR: {avg_psnr:.3f}, Avg SSIM: {avg_ssim:.3f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#saving model weights\n","weights_dir = \"/kaggle/working/\"\n","model_weights_path = os.path.join(weights_dir, 'UNETlatest3.pth')\n","torch.save(model.state_dict(), model_weights_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from IPython.display import FileLink, FileLinks"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["FileLinks('/kaggle/working') #lists all downloadable files on server"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_images_to_plot = 10\n","fig, axes = plt.subplots(num_images_to_plot, 3, figsize=(12, 24))\n","\n","lowres, highres = next(iter((test_dataloader)))\n","\n","lowres = lowres.to(device)\n","outputs = model(lowres)\n","for j in range(num_images_to_plot):\n","\n","\n","    lo = lowres[j].cpu().detach().permute(1, 2, 0).numpy()\n","    output= outputs[j].cpu().detach().permute(1,2,0).numpy()\n","    hi = highres[j].permute(1, 2, 0).numpy()\n","\n","    # Plot noisy image\n","    axes[j, 0].imshow(lo)\n","    axes[j, 0].set_title('LowRes Image')\n","    axes[j, 0].axis('off')\n","\n","    # Plot original image (target)\n","    axes[j, 1].imshow(output)\n","    axes[j, 1].set_title('SR Image')\n","    axes[j, 1].axis('off')\n","\n","    axes[j, 2].imshow(hi)\n","    axes[j, 2].set_title('HighRes Image')\n","    axes[j, 2].axis('off')\n","    \n","\n","# break  # Stop after first batch for demonstration\n","\n","plt.tight_layout()\n","# plt.Arrowshow()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":810739,"sourceId":1388983,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":136214,"modelInstanceId":112888,"sourceId":133515,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
